---
date: 2021-03-30 08:47:29
layout: post
title: "about SVM"
subtitle: "Support Vector Machine 101"
description: "How to find a proper hyperplane"
image: /assets/postimg/SVMLinear3D-1.gif
optimized_image: /assets/postimg/SVMLinear3D-1.gif
category: ML
tags:
    - ML
    - Machine Learning
    - ML Models
math: true
author: Klaus
paginate: false
---



# SVM(Support Vector Machine)



## 특징

- 지도학습 알고리즘
- hyper-plane(초평면)을 이용해 카테고리를 나눈다.
- 분류 시, 마진을 높이는 방향으로 분류 수행
- 하나의 분류 그룹을 다른 그룹과 분리하는 최적의 경계를 찾아내는 알고리즘
- 중간 크기의 데이터셋과 특성(Feature)이 많은 복잡한 데이터셋에서 성능이 좋음



## 결정 경계(초평면)

> 분류를 위한 기준 선(혹은 초평면)

#### Support Vector

> 목적
>
> - margin이 넓은 결정 경계를 만드는 함수를 찾는 것

- 경계를 찾아내는데 기준이 되는 데이터 포인트
- 결정 경계(초평면)에 가장 가까이 있는 vector(데이터포인트)







## Margin

> 결정 경계와 서포트 벡터 사이의 거리

- 두 support vector간의 너비
- 종류
  - hard margin
    - 오차를 허용하지 않는 방식
  - soft margin
    - Overfitting을 방지하기 위해 어느정도 오차를 허용하는 방식





## hyperparameter

>  hyperparameter: `C`

- 노이즈가 있는 데이터나 선형적으로 분리되지 않는 경우 C 값으로 마진을 조정
- default: 1
  - 파라미터값을 크게주면 마진폭이 좁아져 마진 오류가 작아지나 Overfitting이 일어날 가능성이 크다.
  - 파라미터값을 작게 주면 마진폭이 넓어져 마진 오류가 크다. 훈련데이터에서는 성능이 안좋아지나 일반화(generalization)되어 테스트 데이터의 성능이 올라간다. 그러나 underfitting 이 날 가능성이 있다.
  - 언더피팅이 발생 시, 기존 C보다 더 크게 잡아야 한다.
  - 오버피팅이 발생 시, 기존 C보다 줄여야 한다.







## Kernel Support Vector Machine

> 선형으로 분리가 안되는 경우
>
> - 다항식 특성을 추가해 차원을 늘려 선형 분리가 되도록 변환

### 종류

- 선형 서포트 벡터 머신
  - `𝑘(𝑥1,𝑥2)=𝑥𝑇1𝑥2`

- 선형 서포트 벡터 머신
  - `𝑘(𝑥1,𝑥2)=𝑥𝑇1𝑥2k(x1,x2)=x1Tx2`

- 다항 커널 (Polynomial Kernel)
  - `𝑘(𝑥1,𝑥2)=(𝛾(𝑥𝑇1𝑥2)+𝜃)𝑑k(x1,x2)=(γ(x1Tx2)+θ)d`

- RBF(Radial Basis Function) 또는 가우시안 커널(Gaussian Kernel)
  - `𝑘(𝑥1,𝑥2)=exp(−𝛾||𝑥1−𝑥2||2)k(x1,x2)=exp⁡(−γ||x1−x2||2)`

- 시그모이드 커널 (Sigmoid Kernel)
  - `𝑘(𝑥1,𝑥2)=tanh(𝛾(𝑥𝑇1𝑥2)+𝜃)`



### 커널 트릭

> 다항식 특성을 추가하지 않고, 수학적 트릭으로 다항식의 특성을 추가한 것과 같은 결과를 얻을 수 있다.

- 커널 기법은 다항식 차수를 만드는 것이 아니라, 각 샘플을 함수에 넣어 나오는 값으로 다항식 차수를 추가한 것과 같은 효과를 가지게 한다.



#### 차원을 늘리는 경우

- 다항 식의 특성을 추가하는 방법 사용 시,
  - 낮은 차수의 다항식은 데이터의 패턴을 잘 표현하지 못해 언더피팅 문제
  - 너무 높은 차수의 다항식은 과대적합과 모델을 느리게 하는 문제

#### RBF(Radial Base Function, 방사기저) 함수

- Kernel Support Vector Machine의 default kernel function

- 기준점들이 되는 위치를 지정하고, 각 샘플이 그 기준점들과 얼마나 떨어졌는지 유사도(거리) 계산

- 기준점 별 유사도 계산 값은 원래 값보다 차원이 커지고 선형적으로 구분될 가능성이 커진다.
  - $\Phi(x, l) = exp\left(-\gamma \left\|x-l\right\|^2\right)$
  - 방사기저함수 $\Phi$ x: 샘플, l: 기준값, $\gamma$: 직접 정하는 것



- hyperparameter
  - `C`
    - 오차 허용 기준
      - 작은 값일 수록 많이 허용
        - 언더피팅 가능성, 언더피팅인 경우 값을 증가
        - 과소적합이면 너무 오차허용을크게 잡은 것이므로 오차허용을 줄여야 하므로 값을 늘린다. (큰값은 적게 허용)
      - 큰 값일 수록 적게 허용, 오버피팅 가능성
        - 오버피팅인 경우, 값을 감소
        - 과적합이면 훈련셋에 타이트하게 맞춘 것이므로 오차허용을 좀 늘려서 공간을 확보해야 하므로 값을 줄인다. (작은 값일 수록 많이 허용) 
  - `gamma`
    - RBF함수의 $\gamma$로 규제의 역할을 한다.
    - 방사 기저함수 공식상 감마가 크면 반환값은 작아지고 감마가 작으면 반환값은 커진다. ($-\gamma$ 를 곱하므로)
    - 감마가 작을 수록 값들의 거리가 멀어지고(큰값이 결과로 나오므로) 클 수록 거리가 가까워진다. 그래서 gamma 가 크면 거리가 타이트해져 과적합이 일어날 수있다. (공간의 여유가 없므으로)

