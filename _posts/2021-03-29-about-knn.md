---
date: 2021-03-29 08:38:30
layout: post
title: "about KNN"
subtitle:
description:
image: /assets/postimg/knn-main.webp
optimized_image: /assets/postimg/knn-main.webp
category: ML
tags: 
    - ML
    - Machine Learning
    - ML Models
    - KNN
author: Klaus
paginate: false
---


# KNN

> K-Neareest Neighbors
>
> - 특정 포인트에 가장 가까운 k개의 이웃포인트를 참조하여 그 포인트의 y 값을 예측하는 알고리즘
> - 가장 간단한 머신러닝 모델 중 하나
> - 분류 및 회귀 모델에 모두 사용 가능



#### KNN 특징

> - IBL(Instance-Based Learning)
>   - 각각의 관측치(instance)만을 이용하여 새로운 데이터에 대한 예측 진행
> - MBL(Memory-based Learning)
>   - 모든 학습 데이터를 메모리에 저장한 후, 이를 바탕으로 예측 시도
> - Lazy Learning
>   - 학습을 하지 않고, test data가 들어와야 비로소 작동(예측)



#### 예측 알고리즘

1. 예측할 관측치 x를 선택
2. x로부터 인접한 k개의 학습 데이터를 탐색
3. 탐색된 k개 학습 데이터의 Voting값을 x 예측값으로 반환



#### KNN - step

1. k에 해당하는 숫자, 거리 메트릭을 선택
2. 분류하고자하는 샘플에 대한 k개의 근접이웃(nearest neighbors) 찾기
3. Voting 방식으로 분류 레이블 할당



#### KNN - parameter

```python
sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,
                                      weights='uniform',
                                      algorithm='auto',
                                      leaf_size=30,
                                      p=2,
                                      metric='minkowski',
                                      metric_params=None,
                                      n_jobs=None,
                                      **kwargs)
```

> - n_neighbors: k
> - metric: (default='minkowski')
> - p: distance, optional(default=2)
>   - 1: manhattan_distance
>   - 2: euclidean_distance
> - weights
>   - uniform: 모든 거리를 동등하게 계산
>   - distance: 거리의 역수로 계산





#### about 'k'

<img src="https://media.geeksforgeeks.org/wp-content/cdn-uploads/20190523171258/overfitting_2.png" alt="img" style="zoom:50%;" />



#### How to selet `k`

> 일반적으로 cross validation 등을 시행하면서 k 수를 점차 줄여나가며 과적합 여부를 살펴본다. 이때 **Test Error가 최소**가 되는 지점을 찾는다. 

![screenshot](/assets/postimg/knn-crossvalidation.png)





#### Pros & Cons

- 장점

  - 직관적인 모델 : 이해하기 쉽다. / 많이 조정하지 않아도 꽤 괜찮은 성능
  - 데이터에 대한 가정이 없어 비선형 데이터 등에 유용
  - 메모리 기반 방식 :새로운 훈련 데이터를 수집하는 즉시 분류기가 새로운 데이터에 적용(학습과정 x)
  - 데이터 내 노이즈에 큰 영향을 받지 않는다.

  

  

- 한계점

  - 모든 거리계산을 해야하므로 계산 시간이 오래걸린다.
    - 데이터 포인트 수가 많고 차원이 클수록 비용증가

  - outlier에 적절한 대응이 되지 않는다.
  - 메모리에 올려서 사용하므로 높은 메모리가 요구될 수 있음
  - 파라미터 K 값을 직접 설정해줘야한다.
  - 어떤 거리 척도가 분석에 적합한지 불분명, 
    - 데이터 특성에 맞는 distance metric을 임의로 설정해줘야 한다.

  - feature scaling을 해줘야한다. (0.01~0.1 / 1000~10000)









#### tips for reduce overfitting

- Increase training data
- Reduce model complexity
- Early stopping
- Regularization
  - Ridge/Lasso
- drop out